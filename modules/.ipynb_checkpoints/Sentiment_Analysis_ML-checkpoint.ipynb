{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION OF VARIOUS MACHINE LEARNING TECHNIQUES FOR SENTIMENT ANALYSIS.\n",
    "* This notebooks contains the implementation of Multinomial Naive Bayes, LinearSVC, and Random forest for sentiment analysis.\n",
    "* The dataset used is **\"Sentiment Labelled Sentences Dataset\", from the UC Irvine Machine Learning Repository.**\n",
    "* The sentences come from three different websites/fields:\n",
    "    * amazon.com\n",
    "    * imdb.com\n",
    "    * yelp.com\n",
    "* Each sentence is labelled as either 1 (for positive) or 0 (for negative).\n",
    "* For each website, there exist 500 positive and 500 negative sentences.\n",
    "* This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015.  *(Please cite the paper if you want to use it :))*\n",
    "\n",
    "* Link to the dataset is: [Sentiment Labelled Sentences Data Set](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextPreprocessing import TextPreprocessing\n",
    "from Model import train_ml_model\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import ensemble\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS OF AMAZON REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an object of TextPreprocessing class.\n",
    "* This calls contains all the methods for text Preprocesing and vectorizaiton.\n",
    "* Passing \"Amazon\" as the argument to the consturctor indicates that it must use the Amazon product reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessing('Amazon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = tp.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the corpus\n",
    "X = []\n",
    "for c in corpus:\n",
    "    X.append(tp.preprocess_text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training(80%) and testing data(20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not impress would not recommend item anyone\n"
     ]
    }
   ],
   "source": [
    "print(X_train[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering:\n",
    "raw text data will be transformed into feature vectors. The methods used are: \n",
    "1. Count Vectors. (matrix notation representing  the frequency count of a particular term in a particular document)\n",
    "2. Word Level TF-IDF Vectors. (Matrix representing tf-idf scores of every term in different documents)\n",
    "3. N-gram Level TF-IDF Vectors. ( N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams)\n",
    "4. Character Level TF-IDF Vectors. (Matrix representing tf-idf scores of character level n-grams in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count, xvalid_count = tp.count_vectorize(X_train, X_test)\n",
    "xtrain_tfidf, xvalid_tfidf = tp.word_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram, xvalid_tfidf_ngram = tp.n_gram_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars = tp.char_TF_IDF_vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multinomial Naive Bayes for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  0.82\n",
      "Naive Bayes, WordLevel TF-IDF:  0.85\n",
      "Naive Bayes, N-Gram Vectors:  0.61\n",
      "Naive Bayes, CharLevel Vectors:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "NB_cv, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Naive Bayes, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "NB_word_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "NB_n_gram_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Naive Bayes, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "NB_char_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Naive Bayes, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, Count Vectors:  0.82\n",
    "Naive Bayes, WordLevel TF-IDF:  0.85\n",
    "Naive Bayes, N-Gram Vectors:  0.61\n",
    "Naive Bayes, CharLevel Vectors:  0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(NB_cv, '../models/ML/Amazon/NB_cv_amazon.pk1')\n",
    "joblib.dump(NB_word_tf_idf, '../models/ML/Amazon/NB_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_n_gram_tf_idf, '../models/ML/Amazon/NB_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_char_tf_idf, '../models/ML/Amazon/NB_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, Count Vectors:  0.83\n",
      "Random Forest, WordLevel TF-IDF:  0.8\n",
      "Random Forest, N-Gram Vectors:  0.645\n",
      "Random Forest, CharLevel Vectors:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Count Vectors\n",
    "RF_cv, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Random Forest, Count Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Word Level TF IDF Vectors\n",
    "RF_word_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Random Forest, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "RF_n_gram_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Random Forest, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Character Level TF IDF Vectors\n",
    "RF_char_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =200), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Random Forest, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, Count Vectors:  0.83\n",
    "Random Forest, WordLevel TF-IDF:  0.8\n",
    "Random Forest, N-Gram Vectors:  0.645\n",
    "Random Forest, CharLevel Vectors:  0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/RF_char_tf_idf_amazon.pk1']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(RF_cv, '../models/ML/Amazon/RF_cv_amazon.pk1')\n",
    "joblib.dump(RF_word_tf_idf, '../models/ML/Amazon/RF_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_n_gram_tf_idf, '../models/ML/Amazon/RF_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_char_tf_idf, '../models/ML/Amazon/RF_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LinearSVC for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC, Count Vectors:  0.81\n",
      "LinearSVC, WordLevel TF-IDF:  0.84\n",
      "LinearSVC, N-Gram Vectors:  0.715\n",
      "LinearSVC, CharLevel Vectors:  0.825\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC on Count Vectors\n",
    "LinearSVC_cv, accuracy = train_ml_model(LinearSVC(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"LinearSVC, Count Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Word Level TF IDF Vectors\n",
    "LinearSVC_word_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"LinearSVC, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# LinearSVC on Ngram Level TF IDF Vectors\n",
    "LinearSVC_n_gram_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"LinearSVC, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Character Level TF IDF Vectors\n",
    "LinearSVC_char_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram_chars,  xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"LinearSVC, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC, Count Vectors:  0.81\n",
    "LinearSVC, WordLevel TF-IDF:  0.84\n",
    "LinearSVC, N-Gram Vectors:  0.715\n",
    "LinearSVC, CharLevel Vectors:  0.825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/LinearSVC_char_tf_idff_amazon.pk1']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(LinearSVC_cv, '../models/ML/Amazon/LinearSVC_cv_amazon.pk1')\n",
    "joblib.dump(LinearSVC_word_tf_idf, '../models/ML/Amazon/LinearSVC_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_n_gram_tf_idf, '../models/ML/Amazon/LinearSVC_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_char_tf_idf, '../models/ML/Amazon/LinearSVC_char_tf_idff_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuts:\n",
    "* As you can see, count vector, and word-level tf-idf gives the best results for all three models.\n",
    "* Multinomial Naive Bayes seems to be the best model, followed by LinearSVC and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS OF IMDB REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an object of TextPreprocessing class.\n",
    "* This calls contains all the methods for text Preprocesing and vectorizaiton.\n",
    "* Passing \"IMDB\" as the argument to the consturctor indicates that it must use the IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessing('IMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = tp.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for c in corpus:\n",
    "    X.append(tp.preprocess_text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training(80%) and testing data(20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering:\n",
    "raw text data will be transformed into feature vectors. The methods used are: \n",
    "1. Count Vectors. (matrix notation representing  the frequency count of a particular term in a particular document)\n",
    "2. Word Level TF-IDF Vectors. (Matrix representing tf-idf scores of every term in different documents)\n",
    "3. N-gram Level TF-IDF Vectors. ( N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams)\n",
    "4. Character Level TF-IDF Vectors. (Matrix representing tf-idf scores of character level n-grams in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count, xvalid_count = tp.count_vectorize(X_train, X_test)\n",
    "xtrain_tfidf, xvalid_tfidf = tp.word_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram, xvalid_tfidf_ngram = tp.n_gram_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars = tp.char_TF_IDF_vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multinomial Naive Bayes for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  0.82\n",
      "Naive Bayes, WordLevel TF-IDF:  0.85\n",
      "Naive Bayes, N-Gram Vectors:  0.61\n",
      "Naive Bayes, CharLevel Vectors:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "NB_cv, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Naive Bayes, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "NB_word_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "NB_n_gram_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Naive Bayes, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "NB_char_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Naive Bayes, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, Count Vectors:  0.82\n",
    "Naive Bayes, WordLevel TF-IDF:  0.85\n",
    "Naive Bayes, N-Gram Vectors:  0.61\n",
    "Naive Bayes, CharLevel Vectors:  0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/IMDB/NB_char_tf_idf_amazon.pk1']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(NB_cv, '../models/ML/IMDB/NB_cv_amazon.pk1')\n",
    "joblib.dump(NB_word_tf_idf, '../models/ML/IMDB/NB_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_n_gram_tf_idf, '../models/ML/IMDB/NB_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_char_tf_idf, '../models/ML/IMDB/NB_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, Count Vectors:  0.79\n",
      "Random Forest, WordLevel TF-IDF:  0.79\n",
      "Random Forest, N-Gram Vectors:  0.535\n",
      "Random Forest, CharLevel Vectors:  0.77\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Count Vectors\n",
    "RF_cv, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Random Forest, Count Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Word Level TF IDF Vectors\n",
    "RF_word_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Random Forest, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "RF_n_gram_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Random Forest, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Character Level TF IDF Vectors\n",
    "RF_char_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =200), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Random Forest, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, Count Vectors:  0.79\n",
    "Random Forest, WordLevel TF-IDF:  0.79\n",
    "Random Forest, N-Gram Vectors:  0.535\n",
    "Random Forest, CharLevel Vectors:  0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/IMDB/RF_char_tf_idf_amazon.pk1']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(RF_cv, '../models/ML/IMDB/RF_cv_amazon.pk1')\n",
    "joblib.dump(RF_word_tf_idf, '../models/ML/IMDB/RF_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_n_gram_tf_idf, '../models/ML/IMDB/RF_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_char_tf_idf, '../models/ML/IMDB/RF_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LinearSVC for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC, Count Vectors:  0.795\n",
      "LinearSVC, WordLevel TF-IDF:  0.815\n",
      "LinearSVC, N-Gram Vectors:  0.61\n",
      "LinearSVC, CharLevel Vectors:  0.75\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC on Count Vectors\n",
    "LinearSVC_cv, accuracy = train_ml_model(LinearSVC(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"LinearSVC, Count Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Word Level TF IDF Vectors\n",
    "LinearSVC_word_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"LinearSVC, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# LinearSVC on Ngram Level TF IDF Vectors\n",
    "LinearSVC_n_gram_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"LinearSVC, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Character Level TF IDF Vectors\n",
    "LinearSVC_char_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram_chars,  xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"LinearSVC, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC, Count Vectors:  0.795\n",
    "LinearSVC, WordLevel TF-IDF:  0.815\n",
    "LinearSVC, N-Gram Vectors:  0.61\n",
    "LinearSVC, CharLevel Vectors:  0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/IMDB/LinearSVC_char_tf_idff_amazon.pk1']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(LinearSVC_cv, '../models/ML/IMDB/LinearSVC_cv_amazon.pk1')\n",
    "joblib.dump(LinearSVC_word_tf_idf, '../models/ML/IMDB/LinearSVC_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_n_gram_tf_idf, '../models/ML/IMDB/LinearSVC_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_char_tf_idf, '../models/ML/IMDB/LinearSVC_char_tf_idff_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuts:\n",
    "* For IMDB reviews dataset too, you can see, count vector, and word-level tf-idf gives the best results for all three models.\n",
    "* Multinomial Naive Bayes seems to be the best model, followed by LinearSVC and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS OF YELP REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an object of TextPreprocessing class.\n",
    "* This calls contains all the methods for text Preprocesing and vectorizaiton.\n",
    "* Passing \"Yelp\" as the argument to the consturctor indicates that it must use the Yelp restaurant reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessing('Yelp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, labels = tp.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for c in corpus:\n",
    "    X.append(tp.preprocess_text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training(80%) and testing data(20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.20, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering:\n",
    "raw text data will be transformed into feature vectors. The methods used are: \n",
    "1. Count Vectors. (matrix notation representing  the frequency count of a particular term in a particular document)\n",
    "2. Word Level TF-IDF Vectors. (Matrix representing tf-idf scores of every term in different documents)\n",
    "3. N-gram Level TF-IDF Vectors. ( N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams)\n",
    "4. Character Level TF-IDF Vectors. (Matrix representing tf-idf scores of character level n-grams in the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_count, xvalid_count = tp.count_vectorize(X_train, X_test)\n",
    "xtrain_tfidf, xvalid_tfidf = tp.word_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram, xvalid_tfidf_ngram = tp.n_gram_TF_IDF_vectorize(X_train, X_test)\n",
    "xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars = tp.char_TF_IDF_vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multinomial Naive Bayes for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes, Count Vectors:  0.795\n",
      "Naive Bayes, WordLevel TF-IDF:  0.78\n",
      "Naive Bayes, N-Gram Vectors:  0.62\n",
      "Naive Bayes, CharLevel Vectors:  0.76\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "NB_cv, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Naive Bayes, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "NB_word_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Naive Bayes, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "NB_n_gram_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Naive Bayes, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "NB_char_tf_idf, accuracy = train_ml_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Naive Bayes, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, Count Vectors:  0.795\n",
    "Naive Bayes, WordLevel TF-IDF:  0.78\n",
    "Naive Bayes, N-Gram Vectors:  0.62\n",
    "Naive Bayes, CharLevel Vectors:  0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/Yelp/NB_char_tf_idf_amazon.pk1']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(NB_cv, '../models/ML/Yelp/NB_cv_amazon.pk1')\n",
    "joblib.dump(NB_word_tf_idf, '../models/ML/Yelp/NB_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_n_gram_tf_idf, '../models/ML/Yelp/NB_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(NB_char_tf_idf, '../models/ML/Yelp/NB_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest, Count Vectors:  0.8\n",
      "Random Forest, WordLevel TF-IDF:  0.79\n",
      "Random Forest, N-Gram Vectors:  0.61\n",
      "Random Forest, CharLevel Vectors:  0.74\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Count Vectors\n",
    "RF_cv, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"Random Forest, Count Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Word Level TF IDF Vectors\n",
    "RF_word_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"Random Forest, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Random Forest on Ngram Level TF IDF Vectors\n",
    "RF_n_gram_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =100), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"Random Forest, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Random Forest on Character Level TF IDF Vectors\n",
    "RF_char_tf_idf, accuracy = train_ml_model(ensemble.RandomForestClassifier(n_estimators =200), xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"Random Forest, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest, Count Vectors:  0.8\n",
    "Random Forest, WordLevel TF-IDF:  0.79\n",
    "Random Forest, N-Gram Vectors:  0.61\n",
    "Random Forest, CharLevel Vectors:  0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/Yelp/RF_char_tf_idf_amazon.pk1']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(RF_cv, '../models/ML/Yelp/RF_cv_amazon.pk1')\n",
    "joblib.dump(RF_word_tf_idf, '../models/ML/Yelp/RF_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_n_gram_tf_idf, '../models/ML/Yelp/RF_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(RF_char_tf_idf, '../models/ML/Yelp/RF_char_tf_idf_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LinearSVC for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC, Count Vectors:  0.79\n",
      "LinearSVC, WordLevel TF-IDF:  0.8\n",
      "LinearSVC, N-Gram Vectors:  0.65\n",
      "LinearSVC, CharLevel Vectors:  0.8\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC on Count Vectors\n",
    "LinearSVC_cv, accuracy = train_ml_model(LinearSVC(), xtrain_count, xvalid_count, y_train, y_test)\n",
    "print(\"LinearSVC, Count Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Word Level TF IDF Vectors\n",
    "LinearSVC_word_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf, xvalid_tfidf, y_train, y_test)\n",
    "print(\"LinearSVC, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# LinearSVC on Ngram Level TF IDF Vectors\n",
    "LinearSVC_n_gram_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram, xvalid_tfidf_ngram, y_train, y_test)\n",
    "print(\"LinearSVC, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# LinearSVC on Character Level TF IDF Vectors\n",
    "LinearSVC_char_tf_idf, accuracy = train_ml_model(LinearSVC(), xtrain_tfidf_ngram_chars,  xvalid_tfidf_ngram_chars, y_train, y_test)\n",
    "print(\"LinearSVC, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC, Count Vectors:  0.79\n",
    "LinearSVC, WordLevel TF-IDF:  0.8\n",
    "LinearSVC, N-Gram Vectors:  0.65\n",
    "LinearSVC, CharLevel Vectors:  0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/ML/Yelp/LinearSVC_char_tf_idff_amazon.pk1']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(LinearSVC_cv, '../models/ML/Yelp/LinearSVC_cv_amazon.pk1')\n",
    "joblib.dump(LinearSVC_word_tf_idf, '../models/ML/Yelp/LinearSVC_word_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_n_gram_tf_idf, '../models/ML/Yelp/LinearSVC_n_gram_tf_idf_amazon.pk1')\n",
    "joblib.dump(LinearSVC_char_tf_idf, '../models/ML/Yelp/LinearSVC_char_tf_idff_amazon.pk1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuts:\n",
    "* For Yelp reviews dataset too, you can see, count vector, and word-level tf-idf gives the best results for all three models.\n",
    "* Here, LinearSVC seems to be the best model, followed by Random Forest and then Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "* Count vector/ Bog of Words(BOW) and word level tf-idf seems to the best feature engineering vectorizaiton methods for sentiment analysis uing machine learning algorithms.\n",
    "* Multinomial Naive Bayes and LinearSVC outperfrom Random Forest for sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
